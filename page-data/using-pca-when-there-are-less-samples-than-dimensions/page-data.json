{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/using-pca-when-there-are-less-samples-than-dimensions","result":{"data":{"post":{"__typename":"MdxPost","slug":"/using-pca-when-there-are-less-samples-than-dimensions","title":"Using PCA when there are less samples than dimensions","date":"23.04.2018","tags":[{"name":"statistics","slug":"statistics"},{"name":"data-science","slug":"data-science"},{"name":"matlab","slug":"matlab"}],"description":null,"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Using PCA when there are less samples than dimensions\",\n  \"date\": \"2018-04-23T00:00:00.000Z\",\n  \"slug\": \"/using-pca-when-there-are-less-samples-than-dimensions\",\n  \"tags\": [\"statistics\", \"data-science\", \"matlab\"]\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"A significant part of my PhD research \", \"[1,2,5]\", \" has involved the application of PCA (Principal Components Analysis) in unsupervised change detection problems. Working with high-dimensional streaming data, I typically have to work on the assumption that we have no context and no data in advance. We also often need to reduce the dimensionality of the problem - a common application of PCA. We can only work with what we have already seen, so getting your algorithm up and running quickly, even with partial representations of the problem, is of great importance. The code examples in this article will be in MATLAB (sorry) because the tools available make the amount of code minimal. I expect you to already be familiar with what PCA is and what it does (again, sorry, but a full explanation of PCA is way outside the scope of this short post).\"), mdx(\"p\", null, \"So, I want to talk briefly about the application of PCA when you have less samples than dimensions, because the results can be confusing and counter-intuitive to start with. \"), mdx(\"p\", null, \"Let's begin by applying PCA in a normal situation, where we have more samples than dimensions.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"data = randn(1000,50);\\n[coeff, score, latent] = pca(data);\\n\")), mdx(\"p\", null, \"A PCA transformation can be boiled down to \"), mdx(\"p\", null, \"$$\\\\mathbf{T} = \\\\mathbf{X}\\\\mathbf{W}$$\"), mdx(\"p\", null, \"Where $\\\\mathbf{X}$ is the original data, and $\\\\mathbf{W}$ is a matrix of coefficients. In MATLAB terminology,  $\\\\mathbf{X}$ is \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"data\"), \",  $\\\\mathbf{W}$ is \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"coeff\"), \" and $\\\\mathbf{T}$ is \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"score\"), \".\"), mdx(\"p\", null, \"We expect \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"data\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"score\"), \" to have the same dimensions, because it is a transformation of the data into the principal component space. Sure enough, this is reflected in MATLAB.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \">> size(data)\\n\\nans =\\n\\n        1000          50\\n\\n>> size(score)\\n\\nans =\\n\\n        1000          50\\n\\n>> \\n\")), mdx(\"p\", null, \"Now let's take some data that is wider than it is long. We have 50 points in a 1000-dimensional space. Prime real estate for PCA to trim down.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"data = randn(50,1000);\\n[coeff, score, latent] = pca(data);\\n\")), mdx(\"p\", null, \"The same rules apply, right?\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \">> size(data)\\n\\nans =\\n\\n          50        1000\\n\\n>> size(score)\\n\\nans =\\n\\n    50    49\\n\\n>> \\n\")), mdx(\"p\", null, \"Wait, what? Our transformed data only has 49 dimensions instead of 1000. Why? Let's simplify the problem a little. \"), mdx(\"p\", null, \"Imagine instead that we have two points in a 3D space.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"x = rand(2,1);\\ny = rand(2,1);\\nz = rand(2,1);\\nscatter3(x,y,z,'filled');\\n\\nxlabel('x');\\nylabel('y');\\nzlabel('z');\\n\\naxis([0 1 0 1 0 1]); axis square;\\nline([x(1) x(2)],[y(1) y(2)],[z(1) z(2)])\\n\")), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \">> [x,y,z]\\n\\nans =\\n\\n    0.5470    0.7447    0.6868\\n    0.2963    0.1890    0.1835\\n\")), mdx(\"p\", null, mdx(\"span\", _extends({\n    parentName: \"p\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"874px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"89.99999999999999%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAIAAADUsmlHAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA9UlEQVQ4y6WTbQ+DMAiE/f//1ZdErZYC7lwThkzdlvWDMZandz2w2f5Yzf22qorIb7C6Bbi+XMLYY2bDDLDvWO8WXvC6rkHN2/ZeIjzPc9u2tchKC4t34U85wDnncRzDRRPx5phLOKXU9324M7EG5eC8qUkMw+CVZd/WXKSIfGV7mqYAE8t+7bfAIozAIB7SBkhFTvt3gNFMr2w1mdWafAmXUhAYEflJwDsCF9dYPwKHIakbsGDB4FGetVYDDQggoPPZhj5ixxF6ZBAKRtDueA5X53uTiVBUpfAdJyKUD3+VVS/L0nVddYjjzOodbGGG38Os+vUA4sQtcdy2hCEAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"pca 2d\",\n    \"title\": \"pca 2d\",\n    \"src\": \"/static/33953c9cbc90ac854ca42a716fcc0c1d/40a26/pca-2d.png\",\n    \"srcSet\": [\"/static/33953c9cbc90ac854ca42a716fcc0c1d/5243c/pca-2d.png 240w\", \"/static/33953c9cbc90ac854ca42a716fcc0c1d/ab158/pca-2d.png 480w\", \"/static/33953c9cbc90ac854ca42a716fcc0c1d/40a26/pca-2d.png 874w\"],\n    \"sizes\": \"(max-width: 874px) 100vw, 874px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \")), mdx(\"p\", null, \"It's clear when plotting them what the problem is. Principal components are the axes of most variance in the data. But in this case where we only have two points, there's only one axis of variance between them. This exact problem scales up into higher dimensions (but is harder to visualise). If we try to compute the principal component coefficients, we find sure enough that we only have 1 column (one component).\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \">> size(pca([x,y,z]))\\n\\nans =\\n\\n     3     1\\n\")), mdx(\"p\", null, \"The rule is as follows \", \"[3, 4]\", \":\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"A sample of size $n$ with $p$ dimensions has at most $n - 1$ principal components if $n \\\\leq p$. \")), mdx(\"p\", null, \"So effectively, we \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"can\"), \" derive principal components when we have less samples than dimensions, but only $n - 1$ components. In any case this is probably not desirable, because the sample is too small to to derive a fully meaningful transform. If you are planning to use the components to transform future elements of a stream for example, then it is better to wait for at least $p$ samples.\"), mdx(\"h4\", null, \"References\"), mdx(\"p\", null, \"[\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://ieeexplore.ieee.org/abstract/document/6460338/\"\n  }), \"1\"), \"] Kuncheva, Ludmila I., and William J. Faithfull. \\\"Pca feature extraction for change detection in multidimensional unlabelled streaming data.\\\" Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012.\"), mdx(\"p\", null, \"[\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://ieeexplore.ieee.org/abstract/document/6479367/\"\n  }), \"2\"), \"]\\nKuncheva, Ludmila I., and William J. Faithfull. \\\"PCA feature extraction for change detection in multidimensional unlabeled data.\\\" IEEE transactions on neural networks and learning systems 25.1 (2014): 69-80.\"), mdx(\"p\", null, \"[\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stats.stackexchange.com/q/28914\"\n  }), \"3\"), \"] ttnphns (\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stats.stackexchange.com/users/3277/ttnphns\"\n  }), \"https://stats.stackexchange.com/users/3277/ttnphns\"), \"), PCA when the dimensionality is greater than the number of samples, URL (version: 2012-05-22): \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stats.stackexchange.com/q/28914\"\n  }), \"https://stats.stackexchange.com/q/28914\")), mdx(\"p\", null, \"[\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stats.stackexchange.com/q/123318\"\n  }), \"4\"), \"] GrokingPCA (\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stats.stackexchange.com/users/60334/grokingpca\"\n  }), \"https://stats.stackexchange.com/users/60334/grokingpca\"), \"), Why are there only $n-1$ principal components for $n$ data if the number of dimensions is $\\\\ge n$?, URL (version: 2017-10-19): \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://stats.stackexchange.com/q/123318\"\n  }), \"https://stats.stackexchange.com/q/123318\")), mdx(\"p\", null, \"[\", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://www.researchgate.net/profile/Will_Faithfull/publication/328580385_Unsupervised_Change_Detection_in_Multivariate_Streaming_Data/links/5bd70ba74585150b2b8e6b2f/Unsupervised-Change-Detection-in-Multivariate-Streaming-Data.pdf\"\n  }), \"5\"), \"] Faithfull, William. Unsupervised Change Detection in Multivariate Streaming Data. Diss. Bangor University, 2018.\"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"A significant part of my PhD research  1,2,5  has involved the application of PCA (Principal Components Analysis) in unsupervised change…","timeToRead":2,"banner":null}},"pageContext":{"slug":"/using-pca-when-there-are-less-samples-than-dimensions","formatString":"DD.MM.YYYY"}}}