{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/feature-contributions-from-pca","result":{"data":{"post":{"__typename":"MdxPost","slug":"/feature-contributions-from-pca","title":"Feature contributions from PCA","date":"01.11.2018","tags":[{"name":"statistics","slug":"statistics"},{"name":"data-science","slug":"data-science"}],"description":null,"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Feature contributions from PCA\",\n  \"date\": \"2018-11-01T00:00:00.000Z\",\n  \"slug\": \"/feature-contributions-from-pca\",\n  \"tags\": [\"statistics\", \"data-science\"]\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"I had an interesting discussion on \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://www.reddit.com/r/MachineLearning/comments/9px83r/d_anomaly_detection_and_causal_analysis/e8bbr3s\"\n  }), \"/r/MachineLearning\"), \" the other day. The crux was, having applied PCA to some data and measuring some aspect of the principal components, how would we relate that back to the original data? It should be simple in principle, but I found myself scratching my head.\"), mdx(\"p\", null, \"Recalling that we apply a transformation to move the data into the principal component space:\"), mdx(\"p\", null, \"$$\\\\mathbf{T} = \\\\mathbf{X}\\\\mathbf{W}$$ \"), mdx(\"p\", null, \"Then the question is, what is the relationship of $\\\\mathbf{T}$ to $\\\\mathbf{X}$? Specifically, given these three matrices, we'd like to know to what extent each feature in $\\\\mathbf{X}$ contributed to each principal component in $\\\\mathbf{T}$.\"), mdx(\"p\", null, \"Of course, the answer lies in $\\\\mathbf{W}$, our matrix of coefficients. The eigenvalues (\", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"latent\"), \") summarise the amount of variance explained by each principal component. We use this to weight $\\\\mathbf{W}$, and then divide through by the total variance inherent in each variable to attain a percentage. \"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"mu = [1 1 1];\\nsigma = [1 0.5 0.5; 0.5 1 0.5; 0.5 0.5 12];\\ndata = mvnrnd(mu, sigma, 100);\\ndata = mat2gray(data); % Normalise\\n\\n[coeff,~,latent] = pca(data);\\n\\n% Weighted variance\\nvar = coeff .* (repmat(latent',size(coeff,2),1) .* coeff);\\n\\n% Divide through by variable totals\\ntotals = repmat(sum(var,2), 1, size(coeff,1));\\n\\ncontrib = var ./ totals;\\n\")), mdx(\"p\", null, \"Rows of contrib correspond to variables in the original space, columns to principal components. So contrib(m,n) expresses the percentage of variance in variable m explained by principal component n.\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \">> contrib\\n\\ncontrib =\\n\\n    0.0340    0.8170    0.1490\\n    0.0207    0.4759    0.5034\\n    0.9996    0.0004    0.0000\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"I had an interesting discussion on  /r/MachineLearning  the other day. The crux was, having applied PCA to some data and measuring someâ€¦","timeToRead":1,"banner":null}},"pageContext":{"slug":"/feature-contributions-from-pca","formatString":"DD.MM.YYYY"}}}